{
    "comic_number": 2318,
    "explanation": "{{comic\n| number    = 2318\n| date      = June 10, 2020\n| title     = Dynamic Entropy\n| image     = dynamic_entropy.png\n| titletext = Despite years of effort by my physics professors to normalize it, deep down I remain convinced that 'dynamical' is not really a word.\n}}\n\n==Explanation==\nThis is another one of [[Randall|Randall's]] [[:Category:Tips|Tips]], this time a [[:Category:Science tip|Science Tip]]. This time it is a bit special since it came less than three weeks after another Science Tip: [[2311: Confidence Interval]] (which was itself the first time that a non-Protip Tip type has been re-used). This is the first time a type of tip (that was not a [[:Category:Protip|Protip]]) has been used for two \"tips comics\" in a row.\n\nThis Science Tip suggests that if you have a cool new concept, you should call it ''dynamic entropy''.\n\n{{w|Dynamic programming}} is a mathematical optimization method and computer programming method developed by {{w|Richard Bellman}} in the 1950s. The {{w|Dynamic programming#History|History section}} of the Wikipedia article contains the full paragraph from Bellman's autobiography that contains the quote that is in the comic strip. Bellman describes how he was doing mathematical research funded by the military at a time when the Secretary of Defense had a literal pathological fear of the word \"research\", and by extension, \"mathematical\". Bellman borrowed the word \"dynamic\" from physics as being both accurate for his work and as a word that in plain English has positive connotations and is never used in a pejorative sense (expressing contempt or disapproval).  The word \"dynamic\" itself comes from the Greek ''dynamikos'', \"powerful\", which is a positive meaning in itself, and has been applied to topics in physics that are related to motion and forces and used in ordinary English to refer to things that exert power, force, growth, and change (dynamo, dynamite, and as an adjective).  Even though those things aren't always good, when they're bad, we use other words instead (e.g. cancer undergoes {{w|metastasis}}, not \"dynamism\").\n\n{{w|Entropy}} is a term from physics, specifically statistical mechanics, describing a property of a thermodynamic system. When {{w|Claude Shannon}} developed a mathematical framework for studying signal processing and communications systems, which became known as {{w|Information theory}}, he struggled to come up with a proper name for one mathematical concept in his theory that quantified amount of noise or uncertainty in a signal. Computer scientist {{w|John von Neumann}} noticed the similarity of the equations with some in thermodynamics and suggested, \"You should {{w|Entropy (information theory)|call it entropy}}, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage.\" (see {{w|History of information theory#Entropy in statistical mechanics|History of information theory}}). The following is an excerpt from the explanation of [[1862: Particle Properties]]:\n\n<blockquote>\nThe term \"entropy\", which {{w|History of entropy|began}} as a {{w|Entropy (classical thermodynamics)|thermodynamic measure}}, has since been adopted {{w|Entropy in thermodynamics and information theory|by analogy}} into {{w|Entropy (disambiguation)|multiple seemingly unrelated domains}} including, for example, information theory. The table allows that the term \"entropy\" must mean something in the context of particle physics, but isn't certain whether it's the classical, Gibbs' modern {{w|Entropy (statistical thermodynamics)|statistical mechanics}}, Von Neumann's {{w|Von Neumann entropy|quantum entropy}}, or some other meaning. \n\nIn classical thermodynamics, entropy is a macroscopic property describing the disorder or randomness of a system with many particles. However, in statistical mechanics and quantum mechanics, the concept of entropy can also be applied to single particles under certain conditions. If the particle's position is not precisely known and can be described by a probability distribution, this contributes to entropy. Similarly, if the particle's momentum is uncertain and described probabilistically, this also contributes to entropy. A single quantum particle in a pure state (e.g., an electron in a specific atomic orbital) has zero entropy. This is because there is no uncertainty about the state of the system. If the single particle's state is described by a density matrix representing a mixed state (a probabilistic mixture of several possible states), the Von Neumann entropy can quantify the degree of uncertainty or mixedness of the state.\n\nImagine two identical balloons filled with the same gas and heated from two opposite sides with identical heat sources, creating symmetric temperature gradients in both; because the distribution of temperatures is the same, the Gibbs statistical thermodynamic entropy ùëÜ of the gas molecule particles in each balloon will be the same. In contrast, if one balloon is heated by a low-power heat source and another by an otherwise identical high-power heat source, the balloon next to the high-power heat source will have a steeper temperature gradient, increasing the number of [https://www.sciencedirect.com/topics/mathematics/accessible-microstates accessible] {{w|Microstate|microstates}}, so the Gibbs entropy ùëÜ<sub>low_power</sub> < ùëÜ<sub>high_power</sub>. Now consider electrons in two atoms excited by absorbing identical photons to a mixed state; if the mixed states have the same probabilities for different energy levels, their Von Neumann quantum entropy ùëÜ values will be the same. Conversely, if one atom has electrons excited to a {{w|Purity_(quantum_mechanics)|pure state}} and another to a mixed state by photons of different energies, the mixed state will have higher entropy due to greater uncertainty, i.e., ùëÜ<sub>pure</sub> = 0 and 0 < ùëÜ<sub>mixed</sub> ‚â§ ln(2).\n</blockquote>\n\nThe naming of dynamic programming and of entropy in information theory are both examples of scientists choosing a name for what were at least partially very non-scientific seeming reasons. In one case because it has only positive and no negative connotations in plain English. In the other case because there is much confusion over the meaning of the word so Shannon would be free to adopt it in a new context. [[Randall]] is claiming that would make them great to put together to name some new concept; the combination will mean whatever the creator wants it to mean (even able to change mid-debate), and never sound bad the way that e.g. {{w|cold fusion}} has come to be.\n\nEven though the caption implies that \"dynamic entropy\" would be available as a new name, it has actually been used in physics<ref>Allegrini, P., Douglas, J. F., & Glotzer, S. C. (1999). Dynamic entropy as a measure of caging and persistent particle motion in supercooled liquids. Physical Review E, 60(5), 5714, doi: 10.1103/physreve.60.5714.</ref>, probability<ref>Asadi, M., Ebrahimi, N., Hamedani, G., & Soofi, E. (2004). Maximum Dynamic Entropy Models. Journal of Applied Probability, 41(2), 379-390. Retrieved June 11, 2020, from www.jstor.org/stable/3216023</ref>, computer science<ref>S. Satpathy et al., \"An All-Digital Unified Static/Dynamic Entropy Generator Featuring Self-Calibrating Hierarchical Von Neumann Extraction for Secure Privacy-Preserving Mutual Authentication in IoT Mote Platforms,\" 2018 IEEE Symposium on VLSI Circuits, Honolulu, HI, 2018, pp. 169-170, doi: 10.1109/VLSIC.2018.8502369.</ref>, and even the term \"dynamical entropy\" in physics<ref>Green, J. R., Costa, A. B., Grzybowski, B. A., & Szleifer, I. (2013). Relationship between dynamical entropy and energy dissipation far from thermodynamic equilibrium. Proceedings of the National Academy of Sciences, 110(41), 16339-16343.</ref><ref>S≈Çomczy≈Ñski, W., & Szczepanek, A. (2017). Quantum dynamical entropy, chaotic unitaries and complex Hadamard matrices. IEEE Transactions on Information Theory, 63(12), 7821-7831, doi: 10.1109/TIT.2017.2751507.</ref> and bioscience<ref>Chakrabarti, C. G., & Ghosh, K. (2013). Dynamical entropy via entropy of non-random matrices: Application to stability and complexity in modelling ecosystems. Mathematical biosciences, 245(2), 278-281, doi: 10.1016/j.mbs.2013.07.016.</ref>.\n\nIn the title text Randall mentions that, even though his physics professors have continued to use the word \"dynamical\", \"trying to normalize it\" by repetitive usage, he remains convinced that it is not really a word.  Presumably he doesn't like that it has two suffixes used to make words into adjectives, -ic and -al, as if \"dynamic\" wasn't already positive enough. The [https://www.thefreedictionary.com/Commonly-Confused-Suffixes-ic-vs-ical.htm#:~:text=Words%20ending%20in%20%E2%80%9C%2Dic%E2%80%9D,are%20notoriously%20difficult%20to%20distinguish Free Dictionary] discusses how -ic and -ical suffixes are confused in many common words and explains their different uses.\n\nThe term \"dynamical\" in physics generally is used in \"{{w|Dynamical system}}\" or as an adjective to name a concept as applied to dynamical systems such as \"dynamical entropy\"<ref>Atmanspacher, H. (1997) \"Dynamical entropy in dynamical systems,\" in ''Time, temporality, now'' (pp. 327-346). Springer, Berlin, Heidelberg, doi: 10.1007/978-3-642-60707-3_22</ref>.\n\n==Transcript==\n:[One panel only with text and a few lines and arrows. There are two columns each with a heading. Beneath each heading is a quote written on four lines. Below the quote, in grey font, and indented, starting with a hyphen, with the text aligned to the right of this are five lines of text. This explains who the quote belongs to and where it was stated (in brackets at the end). From the bottom of each of these two gray text paragraphs gray curved arrows goes down to two gray lines. Below each of these two lines are one large word per line. They are again in black text.]\n:<big>Dynamic</big>\n:\"It's impossible to use the word 'dynamic' in the pejorative sense... Thus, I thought 'Dynamic Programming' was a good name.\"\n::<font color=\"gray\">- Richard Bellman, explaining how he picked a name for his math research to try to protect it from criticism (''Eye of the Hurricane'', 1984)</font>\n\n:<big>Entropy</big>\n:\"You should call it 'Entropy'... No one knows what entropy really is, so in a debate you will always have the advantage.\"\n::<font color=\"gray\">- John von Neumann, to Claude Shannon, on why he should borrow the physics term in information theory (as told to Myron Tribus)</font>\n\n:::<big><big>'''''Dynamic Entropy'''''</big></big>\n\n:[Caption below the panel:]\n:Science Tip: If you have a cool concept you need a name for, try \"Dynamic Entropy.\"\n\n== Trivia ==\n\nMany of {{w|Buckminster Fuller}}'s designs and works were associated with the word \"{{w|dymaxion}}\", a combination of the words \"dynamic\", \"maximum\", and \"tension\", all words that Fuller himself used a lot in talking about his work, and which are words that simultaneously have use in science and positive connotations in lay English.\n\n==References==\n<references />\n\n{{comic discussion}}\n\n[[Category:Science tip]]\n[[Category:Comics featuring real people]]\n[[Category:Science]]\n[[Category:Physics]]\n[[Category:Programming]]"
}