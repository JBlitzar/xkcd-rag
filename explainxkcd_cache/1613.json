{
    "comic_number": 1613,
    "explanation": "{{comic\n| number    = 1613\n| date      = December 7, 2015\n| title     = The Three Laws of Robotics\n| image     = the_three_laws_of_robotics.png\n| titletext = In ordering #5, self-driving cars will happily drive you around, but if you tell them to drive to a car dealership, they just lock the doors and politely ask how long humans take to starve to death.\n}}\n\n==Explanation==\nThis comic explores alternative orderings of sci-fi author {{w|Isaac Asimov|Isaac Asimov's}} famous {{w|Three Laws of Robotics}}, which are designed to prevent robots from taking over the world, etc. These laws form the basis of a number of Asimov works of fiction, including most famously, the short story collection ''{{w|I, Robot}}'', which amongst others includes the very first of Asimov's stories to introduce the three laws: {{w|Runaround (story)|Runaround}}.\n\nThe three rules are:\n#A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n#A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\n#A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n\nIn order to make his joke, [[Randall]] shortens the laws into three imperatives:\n#Don't harm humans\n#Obey orders\n#Protect yourself\n\nAnd then implicitly adds the following to the end of each law regardless of order of imperatives:\n#''[end of statement]''\n#_____, except where such orders/protection would conflict with the First Law.\n#_____, as long as such orders/protection does not conflict with the First or Second Laws.\n\nThis comic answers the generally unasked{{citation needed}} question: \"Why are they in that order?\" With three rules you could rank them into 6 different {{w|permutation|permutations}}, only one of which has been explored in depth. The original ranking of the three laws are listed in the brackets after the first number. So in the first example, which is the original, these three numbers will be in the same order. For the next five the numbers in brackets indicate how the laws have been re-ranked compared to the original.\n\nThe comic begins with introducing the original set, which we already know will give rise to a balanced world, so this is designated as green.:\n;Ordering #1 - <font color=\"green\">Balanced World</font>: The safety of humans is placed as the top priority, superseding even a robot's preprogrammed obedience; a robot may disregard any orders they are given if that would result in harm to humans, but otherwise must obey all instructions. The \"inaction\" clause ensures that a robot will actively save humans in danger, and also not {{w|Little Lost Robot|place humans in hypothetical danger}} and then leave them to that fate. Their own self-preservation is placed at the lowest priority, which means they will sacrifice themselves if necessary to save a human life, and must obey orders even if they know those orders will result in their own destruction. This results in a balanced, if not perfect, world. Asimov's robot stories explore in detail the ramifications of this scenario.\n\nBelow this first known option, the five alternative orderings of the three rules are illustrated. Two of the possibilities are designated yellow (pretty bad or just annoying) and three of them are designated red (\"Hellscape\").\n\n;Ordering #2 - <font color=\"orange\">Frustrating World</font>: Human safety is still top priority, so there is no danger to humans; however, the priority of self-preservation is now placed above obedience, which means that the robots value their existence over their job and so many would refuse to do their tasks. The silliness of this is portrayed in the accompanying image, where the robot (a {{w|Mars rover}} looking very similar to {{w|Curiosity (rover)|Curiosity}} both in shape and size - see [[1091: Curiosity]]) laughs at the idea of doing what it was clearly built to do (explore {{w|Mars}}) because of the risk. In addition to the general risk (e.g. of unexpected damage), it is actually normal for rovers to cease operating (\"die\") at the end of their mission, though they may survive longer than expected (see [[1504: Opportunity]] and [[695: Spirit]]).\n;Ordering #3 - <font color=\"red\">Killbot Hellscape</font>: This puts obeying orders above not harming humans, which means anyone could send a robot on a killing spree. Given human nature, it will probably only be a matter of time before this happens. Even worse, if the robot prioritizes obeying orders above human safety, it may try to kill any human who would prevent it from fulfilling those orders, even the person who originally gave them. Given the superior abilities of robots, the most effective way to stop them would be to counter them with other robots, which would quickly escalate to a \"Killbot Hellscape\" scenario where robots kill indiscriminately without any thought for human life or self-preservation.\n;Ordering #4 - <font color=\"red\">Killbot Hellscape</font>: This is much the same as #3, except even worse as robots would also be able to kill humans in order to protect themselves. This means that even robots not engaged in combat might still murder humans if their existence is threatened. It would be a very dangerous world for humans to live in. To be fair, a robot attacking a human purely in self-defense could be ordered to stop, which it would be forced to obey. However, there are situations where such an order could not be given: the robot uses a sneak attack, the human cannot speak coherent sentences, etc. And still retains the possibility of other humans having actually ordered it to kill, in the first place, with no clearly overriding way of ever countermanding this, as with each example where human life is subordinate to the obeying of (other) humans.\n;Ordering #5 - <font color=\"orange\">Terrifying Standoff</font>:This ordering would result in an unpleasant world, though not necessarily a full Hellscape. Here the robots would not only disobey to protect themselves, but also kill if necessary. The absurdity of this one is further demonstrated with the very un-human robot happily doing repetitive mundane tasks but then threatening the life of its user, [[Cueball]], if he as much as considers unplugging it. Interestingly, this makes robots the most similar to humans out of all of the other scenarios.\n;Ordering #6 - <font color=\"red\">Killbot Hellscape</font>: The last ordering puts self-protection first, which allows robots to go on killing sprees as long as doing so wouldn't cause them to come to harm. While not as bad as the Hellscapes in #3 and #4, this is still not good news for humans, as a robot can easily kill a human without risk to itself. A human also cannot use a robot to defend it from another robot, as robots can refuse combats that involve risk to themselves - this means a robot would happily stand by and allow its human master to be killed. According to Randall, this still eventually results in the Killbot Hellscape scenario.\n\nThe title text shows a further horrifying consequence of ordering #5 (\"Terrifying Standoff\"), by noting that a self-driving car could elect to kill anyone wishing to trade it in. Since cars aren't designed to kill humans, one way it could achieve this without any risk to itself is by locking the doors (which it would likely have control over, as part of its job) and then simply doing nothing at all. Humans require food and water to live, so denying the passenger access to these will eventually kill them, removing the threat to the car's existence. This would result in a horrible, drawn-out death for the passenger, if they cannot escape the car. It should be noted that although the car asked how long humans take to starve, the human would die of dehydration first. In his original formulation of the First Law, Asimov created the \"inaction\" clause specifically to avoid scenarios in which a robot puts a human in harm's way and refuses to save them; this was explored in the short story {{w|Little Lost Robot}}.\n\nAnother course of action by an AI, completely different than any of the ones presented here, is depicted in [[1626: Judgment Day]].\n\n==Transcript==\n:[Caption at the top of the comic:]\n:'''Why Asimov put the Three Laws'''\n: '''of Robotics in the order he did.'''\n\n:[Below are six rows with first two frames and then a label in color to the right. Above the two column of frames there are labels as well. In the first column six different ways of ordering the three laws are listed. Then the second column shown an image of the consequences of this order. Except in the first where there is a reference. The label to the right rates the kind of world that order of the laws would result in.]\n\n:[Labels above the columns.]\n:Possible ordering\n:Consequences\n\n:[The six rows follows below. First the text in the first frame, then a description of the second frame, including possible text below and finally the colored label.]\n\n:[First row:]\n:1. (1) Don't harm humans\n:2. (2) Obey Orders\n:3. (3) Protect yourself\n:[Only text in square brackets:]\n::[See Asimov’s stories]\n:<font color=\"green\">'''Balanced world'''</font>\n\n:[Second row:]\n:1. (1) Don't harm humans\n:2. (3) Protect yourself\n:3. (2) Obey Orders\n:[Megan points at a mars rover with six wheels, a satellite disc, an arm and a camera head turned towards her, what to do.]\n:Megan: Explore Mars!\n:Mars rover: Haha, no. It’s cold and I’d die.\n:<font color=\"orange\">'''Frustrating world'''</font>\n\n:[Third row:]\n:1. (2) Obey Orders\n:2. (1) Don't harm humans\n:3. (3) Protect yourself\n:[Two robots are fighting. The one to the left has six wheels, a tall neck on top of the body, with a head with what could be a camera facing right. It has something pointing forward on the body, which could be a weapon. The robot to the right, seems to be further away into the picture. (it is smaller with less detail). It is human shapes, but made op of square structures. It has two legs and two arms, a torso and a head. It clearly shoots something out of it’s right “hand”. This shot seems to create an explosion a third of the way towards the left robot. There are two mushroom clouds from explosions behind both robots (left and right). Between them there are one more explosion up in the air close to the left robot, and what looks like a fire on the ground right between them. Furthermore there are two missiles in the air, one above the head of each robot. Lines indicate their trajectory. There is not text.]\n:<font color=\"red\">'''Killbot hellscape'''</font>\n\n:[Fourth row:]\n:1. (2) Obey Orders\n:2. (3) Protect yourself\n:3. (1) Don't harm humans:\n:[Two robots are fighting. The one to the left has six wheels, a tall neck on top of the body, with a head with what could be a camera facing right. It has something pointing forward on the body, which could be a weapon. The robot to the right, seems to be further away into the picture. (it is smaller with less detail). It is human shapes, but made op of square structures. It has two legs and two arms, a torso and a head. It clearly shoots something out of it’s right “hand”. This shot seems to create an explosion a third of the way towards the left robot. There are two mushroom clouds from explosions behind both robots (left and right). Between them there are one more explosion up in the air close to the left robot, and what looks like a fire on the ground right between them. Furthermore there are two missiles in the air, one above the head of each robot. Lines indicate their trajectory. There is not text.]\n:<font color=\"red\">'''Killbot hellscape'''</font>\n\n:[Fifth row:]\n:1. (3) Protect yourself\n:2. (1) Don't harm humans\n:3. (2) Obey Orders\n:[Cueball is standing in front of a car factory robot, that is larger than him. It has a base, and two parts for the main body, and then a big “head” with a small section on top. To the right something is jutting out, and to the left in the direction of Cueball there is an arm in three sections (going down, up and down again) ending in some kind of tool close to Cueball.]\n:Car factory robot: I'll make cars for you, but try to unplug me and I’ll vaporize you.\n:<font color=\"orange\">'''Terrifying standoff'''</font>\n\n:[Sixth row:]\n:1. (3) Protect yourself\n:2. (2) Obey Orders\n:3. (1) Don't harm humans:\n:[Two robots are fighting. The one to the left has six wheels, a tall neck on top of the body, with a head with what could be a camera facing right. It has something pointing forward on the body, which could be a weapon. The robot to the right, seems to be further away into the picture. (it is smaller with less detail). It is human shapes, but made op of square structures. It has two legs and two arms, a torso and a head. It clearly shoots something out of it’s right “hand”. This shot seems to create an explosion a third of the way towards the left robot. There are two mushroom clouds from explosions behind both robots (left and right). Between them there are one more explosion up in the air close to the left robot, and what looks like a fire on the ground right between them. Furthermore there are two missiles in the air, one above the head of each robot. Lines indicate their trajectory. There is not text.]\n:<font color=\"red\">'''Killbot hellscape'''</font>\n\n{{comic discussion}}\n\n[[Category:Comics with color]]\n[[Category:Comics featuring Cueball]]\n[[Category:Comics featuring Megan]]\n[[Category:Artificial Intelligence]]\n[[Category:Robots]]\n[[Category:Mars rovers]]"
}