{
    "comic_number": 2739,
    "explanation": "{{comic\n| number    = 2739\n| date      = February 17, 2023\n| title     = Data Quality\n| image     = data_quality_2x.png\n| imagesize = 671x211px\n| noexpand  = true\n| titletext = [exclamation about how cute your cat is] -> [last 4 digits of your cat's chip ID] -> [your cat's full chip ID] -> [a drawing of your cat] -> [photo of your cat] -> [clone of your cat] -> [your actual cat] -> [my better cat]\n}}\n\n==Explanation==\n<!-- Specifically \"No Idea If There's A Character Limit LMAO\": please refrain from removing any more Incomplete tags by yourself and so quickly, and please check your Talk page! And please remove this comment once you've read it. :) -->\n\nDigital data can be compressed to make transmission and/or storage more efficient; some {{w|compression algorithms}} discard some information to improve the compression, which is known as lossy compression, since some of the information is lost (this can be acceptable in audio or visual data, since the difference may be hard for humans to perceive).\n\nThis comic shows a chart in the form of a line, increasing quality from very lossy to most lossless. This means that it goes, at the extremes, from having so little information as to make it effectively meaningless, to having significant extra information included (eventually making the original actually an unnecessary distraction). Some of this extra information mitigates the risk of another sense of 'loss' in data - digital data are transferred in bits, and {{w|data loss}} is the process by which some of these bits are lost or altered during data transport. However the highest quality, \"better data\", is using a different sense of the term \"quality\", referring more to the general excellence of the data than how accurately it represents the original.\n\nThe title text uses your cat as an example of this range of losses (or, in the case of the latter reaches of the graph, gains) in the information. This is possibly a reference to [https://www.goodreads.com/quotes/8157292-the-best-material-model-of-a-cat-is-another-or Norbert Wiener]'s quote, \"The best material model of a cat is another, or preferably the same, cat.\" The most lossy is an exclamation about how cute your cat is, which is ephemeral and obviously carries very little significance in terms of actually providing specific, transferable information about your cat. The example then progresses into your cat's chip ID; presumably your cat has been microchipped, and between the last four digits (commonly used in sensitive information as an identifier without revealing the full number) or the entire chip ID, provides a still-uninformative yet slightly improved way of identifying your cat. A drawing of your cat and a photo of your cat would portray the cat reasonably well, while a clone of your cat and (of course) your actual cat would be the best way of gaining information about your cat. However, as in the actual comic, the final, most lossless (in this case, with the most gain) form of data transfer has nothing to do with your cat, but is simply Randall's better cat. This is apparently made out by Randall to be the pinnacle of cat data.\n\n=== Details ===\n{| class=\"wikitable\" \n|-\n! Item\n! Title Text\n! Explanation\n|-\n| Someone who once saw the data describing it at a party\n| exclamation about how cute your cat is\n| This is referring to how unreliable and inaccurate it is to get information verbally second-hand, as humans are naturally terrible at maintaining accuracy when passing on information received. This is the basic premise behind {{w|Chinese whispers|the Telephone Game}}. People naturally and instinctively mentally summarize information received in the way they understand, often in their own words instead of what they literally heard or read.\n|-\n| {{w|Bloom filter}}\n| last 4 digits of your cat's chip ID\n| A Bloom filter is a probabilistic data structure that can efficiently say whether an element is ''probably'' part of the dataset, while it can say \"element is not in set\" with 100% accuracy. If a Bloom filter is used to represent the contents of a book, reference to the Bloom filter could perhaps reconstruct everything, just by guessing, but in a highly inefficient and potentially inaccurate way. A bloom-filter is like a the last four digits of the cat's ID in that while you can know for sure a cat isn't your cat if it's last four digits don't match, you can't know for sure that it is yours if they do.\n|-\n| {{w|Hash table}}\n| your cat's full chip ID\n| A hash table allows you to find data very fast. Calculating a hash value for a piece of data means that there is (most probably) a unique relationship between the data and a hash value - e.g. \"58b8893b172d00e9\". This means this exact version of the data will yield this exact hash value, though it's practically impossible to reconstruct the data from a hash value. It is a method of checking that a copy is the same as the original, but is meaningless on its own and has the possibility of being wrong. For example, an average book contains several millions of bits, yet the SHA-2 hash has only 256 bits, so there are theoretically many (mostly nonsensical, but not necessarily) 'wrong' books that might look correct under a hash function.\n|-\n| {{w|JPEG|JPG}}, {{w|GIF}}, {{w|MPEG-1|MPEG}}\n| a drawing of your cat\n| Image, audio, and video formats that are considered '{{w|Lossy compression|lossy}}'. The JPG (or \"JPEG\") picture format and the {{w|MPEG}} group of audio and video formats ({{w|MP1}}, {{w|MPEG-1 Audio Layer II|MP2}}, {{w|MP3}}, {{w|MP4}}, {{w|Advanced Audio Coding|AAC}}) typically use a range of data-compression methods that save space by selectively fudging (thus losing) what details it can of the image and audio, to make disproportionate gains in compression ratios; best used for real world images (and films) where real-world 'noise' can afford to be replaced by a more compressible version, without too much obvious change.\nGIF compression is not 'lossy' in the same way, i.e. whatever it is asked to encode can be faithfully decoded, but Randall may consider its limitations (it can only write images of 256 unique hues, albeit that these can come from anywhere across the whole 65,536 \"True color\" range, plus transparency) to be a form of loss, as conversion from a more sophisticated format (e.g. PNG, below) could lose many of the subtle shades of the original and produce an inferior image. For this reason, GIF format becomes one best left to render diagrams and other computer-generated imagery with swathes of identical pixels and mostly sharp edges (and to utilize the optional transparent mask), for which JPEG compression will create prominent image artefacts. Alternatively, he may just have included it as a joke/nerd-snipe.\n|-\n| {{w|PNG}}, {{w|ZIP (file format)|ZIP}}, {{w|TIFF}}, {{w|WAV}}, raw data\n| photo of your cat\n| A series of formats using {{w|lossless compression}}. PNG and TIFF are image formats that are suitable for photos, but without (necessarily) resorting to reduced accuracy in order to assist compression. WAV is an audio format that stores the raw audio samples uncompressed, unlike compressed formats like {{w|FLAC}} which can reduce the size of an WAV file by 50% while still being lossless.\nZIP is a generic compression algorithm (and the name of the format it creates) that can be used to store any other digital files. Anything put within a ZIP file can be exactly decompressed into the original state later on, although any such file already compressed in some way (such as any of the image formats mentioned in this comic, or other ZIPs) are unlikely to compress significantly more. Additionally, {{w|Lossless compression#Limitations|it must also make some data larger}} due to the {{w|Pigeonhole Principle}} and the fact that it compresses losslessly.\n|-\n| Raw data + parity bits for error detection\n| clone of your cat\n| In any data transmission, there is the possibility for errors in the data received. Naturally, there are methods to detect a (limited) number of errors. A simple example are {{w|parity bits}}. Say we wanted to transmit the number 181. In the binary representation of 181 (10110101), there are an odd number of '1's. A simple algorithm might append a bit so that the number of ones in the transmitted message is even, so '101101011' would be transmitted instead of '10110101'. At the receiver, the number of ones in the received message can be summed to determine if an error occurred in the message. If an odd number of ones were received, say '101101010', then the receiver knows that an error occurred because the transmitted signal should never contain an odd number of ones due to the parity bit that was added. Such a method can detect single bit errors, but cannot detect 2-bit errors, because the parity of the message would change twice, swapping back to what it was before.\nThere are more reliable means to detect errors, such as {{w|Cyclic redundancy check|CRC-32}}, {{w|MD5}} and the much more modern {{w|Secure Hash Algorithm|SHA}}. Such values were alluded to in the Hash Table section. But here they are sent ''alongside'' the data, slightly increasing the amount of data transmitted/stored (in order to establish its accuracy), rather than instead of it and vastly decreasing the amount of 'necessary' data (but leaving the virtually impossible task of performing a correct reconstruction).\nHowever it is done, if the check indicates a problem then you can only request a new copy of the data, hoping that the problems encountered in the transmission can be resolved.\n|-\n| Raw data + parity bits for error ''correction''\n| your actual cat\n| With {{w|Error correction code|error correction codes}}, there are ways to immediately restore the original data with the given additional data in the event of noise in the transmission medium. One method is to 'overlap' multiple error-detection parities such that any small enough corruption of data (including of parity bits themselves) can be reconstructed to the correct original value by cross-comparison between all parity bits and the supposed data. One of the first modern methods developed was {{w|Hamming(7,4)}}, invented around 1950, which was a balanced approach designed to handle the typical error conditions typically encountered at the time and has inspired even contemporary electronic methods of maintaining data integrity. Another practical application of error correction bits would be that present in {{w|QR_code#Error_correction|QR Codes}} and {{w|Cross-interleaved_Reed–Solomon_coding|CDs}}, both using {{w|Reed–Solomon error correction|Reed–Solomon error correction}}.\n|-\n| Better data\n| my better cat\n| All of the previous examples dealt with preserving the quality of the recorded data. However, {{w|Garbage in, garbage out|they can do nothing if the recorded data are bad}}. If the data are of bad quality, then adding parity bits will ensure that the bad quality data is what is recovered. The only way to increase the quality of the extracted data is to start 'better' data.\nFor example, if a .WAV file is converted into a .MP3 file and back, some data quality will be lost. No amount of processing will be able to recover exactly the data that was lost in the conversion. The only way to recover the lost quality is to obtain the 'better' data of the original file.\n|}\n\n==Transcript==\n:[A line chart is shown with eight unevenly-spaced ticks each one with a label beneath the line. Above the middle of the line there is a dotted vertical line with a word on either side of this divider. Above the chart there is a big caption with an arrow beneath it pointing right.]\n:<big>Data Quality</big>\n:Lossy ┊ Lossless\n\n:[Labels to the left of the dotted line from left to right:]\n:Someone who once saw the data describing it at a party\n:Bloom filter\n:Hash table\n:JPEG, GIF MPEG\n\n:[Labels to the right of the dotted line from left to right:]\n:PNG, ZIP, TIFF, WAV, Raw data\n:Raw data + parity bits for error detection\n:Raw data + parity bits for error ''correction''\n:Better data\n\n{{comic discussion}}\n\n[[Category:Charts]]\n[[Category:Cats]]"
}